{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Challenge - Part-II - Report : by Debisree Ray\n",
    "\n",
    "\n",
    "##  1. The Data:\n",
    "\n",
    "The data contains the user action logs from a popular online retail website, captured for 14 days between 2016-06-01 to 2016-06-14 (both days inclusive). Columns in the dataset are follows:\n",
    "\n",
    "* **userid:** unique identifier of user who visited the website\n",
    "* **offerid:** unique identifier of the offer shown\n",
    "* **countrycode:** two-character country code\n",
    "* **category:** category ID of the offer\n",
    "* **merchant:** unique identifier of the merchant who has published the offer\n",
    "* **utcdate:** timestamp of the user action\n",
    "* **rating:** if the user has clicked the offer or not (1:clicked, 0: not clicked, only viewed)\n",
    "\n",
    "\n",
    "##  1.a.Questions:\n",
    "\n",
    "1. Think about a situation, where a mobile advertisement company has this historical data. Each impression (placing advertisement) cost the advertisement company 1 cent, and each click cost the advertisement company 1USD,(1USD=100 cents). Each {userid, offerid, merchantid} should have 10 impressions. It has been given by merchants (the companies who have contracted with the advertisement company to run the advertisement campaign) that for each impression the ROI (return on investment) for the merchants is 10 cents and for each click the ROI for the merchant is 10USD. The advertisement company has 10,000 USD to run the advertisement campaign in the next 7 days. Based on the above historical dataset could you identify the {userid, offerid, merchantid} combination (or combinations) that the advertisement agency should target in this campaign? Please clearly narrate your intuition and process behind choosing the combinations.\n",
    "\n",
    "2. Develop at least two models which will predict whether the advertisement will be clicked or not. (***rating*** is the dependent variable). Provide detailed reports behind choosing different parameters in building your models by comments in your code. Produce the relevant validation metrics for training and testing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Data wrangling:\n",
    "\n",
    "* To start, we need to first import all the necessary modules and libraries.\n",
    "* The imp. libraries include:\n",
    "   * **NumPy:** Provides a fast numerical array structure and helper functions.\n",
    "   * **pandas:** Provides a DataFrame structure to store data in memory and work with it easily and efficiently.\n",
    "   * **scikit-learn:** The essential Machine Learning package in Python.\n",
    "   * **matplotlib:** Basic plotting library in Python; most other Python plotting libraries are built on top of it.\n",
    "   * **Seaborn:** Advanced statistical plotting library.\n",
    "   \n",
    "* Read the train/test data set into the **'Pandas dataframe'**.\n",
    "\n",
    "* There are 7 columns and 15844717 rows in the training data.\n",
    "\n",
    "* There are 7 columns and 1919561 rows in the test data. The first few lines of the train and the test data sets look as follows:\n",
    "\n",
    "<img src=\"train.png\" align=\"center\" width=\"90%\"/>\n",
    "<img src=\"test.png\" align=\"center\" width=\"90%\"/>\n",
    "\n",
    "* The columns are: **'userid', 'offerid', 'countrycode', 'category', 'merchant', 'utcdate', 'rating'**.\n",
    "* There is no missing value in the columns.\n",
    "* The column **'countrycode' = 'de'** for the entire dataset. So, We can get rid of the column, as that has nothing to do with the modeling.\n",
    "* The target variable is **'rating'**, which can take the value either 1/0 (depending on whether the link has been clicked or not.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Exploratory Data Analysis (EDA):\n",
    "\n",
    "* To start the EDA, here every different features have been studied and visually displayed against the target variable, so as to infer any relationship between them.\n",
    "\n",
    "###  3.1. userid:\n",
    "\n",
    "* This is the unique identifier of user who visited the website.\n",
    "* Total 291485 users.\n",
    "* There are some users who are frequent users (visited the website at least twice).\n",
    "* There are some users who are actually one-timers (did not return to the website)\n",
    "* 48.9% of the total users are non-returning, who clicked.\n",
    "* 0.5% of the total users are non-returning, who has not clicked.\n",
    "* 28.9% of the total users are returning, who has not clicked.\n",
    "* 21.7% of the total users are returning, who clicked.\n",
    "\n",
    "<img src=\"1.png\" align=\"left\" width=\"50%\"/>\n",
    "<img src=\"2.png\" align=\"right\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. offerid:\n",
    "\n",
    "* This is the unique identifier of the offer shown.\n",
    "* There are 2158859 offer-IDs listed.\n",
    "* Here we have shown the distribution of the offer IDs for both clicked and not clicked.\n",
    "\n",
    "<img src=\"3.png\" align=\"center\" width=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.3. category :\n",
    "\n",
    "* These are the different categiroes (IDs) for different offer IDs\n",
    "* 271 unique different offer categories are there.\n",
    "* Maximum frequency for an offer category = 934537\n",
    "* Minimum frequency for an offer category = 15\n",
    "* Here we have shown the distribution of the categories for both clicked and not clicked.\n",
    "\n",
    "<img src=\"4.png\" align=\"center\" width=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.4. merchantid:\n",
    "\n",
    "* unique identifier of the merchant who has published the offer.\n",
    "* 703 different merchants are there.\n",
    "* Here we have shown the distribution of the merchant IDs for both clicked and not clicked.\n",
    "\n",
    "<img src=\"5.png\" align=\"center\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.5. utcdate:\n",
    "\n",
    "* This is the timestamp of the user action.\n",
    "* Here we have splitted the timestamp into further details.\n",
    "* The additional columns created are: \n",
    "   * **'dayofweek'**: Here we have plotted the user activities over the days of the week. And on the same graph, we have shown the clicked ones. We can see that the maximum activities are on Fridays. However, 30.5% of the clicks are done on the Tuesdays.\n",
    "   * **'date'**: Here we have plotted the activities over the dates. And on the same graph plotted the clicked ones. We see that the maximum activity is on 12th. However maximum clicks are on 14th.\n",
    "   * **'Hour'**: Here we have plotted the user activities over the hours of days. In two different graphs we have shown the clicks/ not click activity rates. Clearly, the click rates are maximum at the 17th hour.\n",
    " \n",
    "* Here we have shown the distribution of the dayofweek, date and hour for both clicked and not clicked.\n",
    "\n",
    "<img src=\"6.png\" align=\"left\" width=\"50%\"/> <img src=\"7.png\" align=\"right\" width=\"50%\"/>\n",
    "<img src=\"8.png\" align=\"left\" width=\"50%\"/> <img src=\"9.png\" align=\"right\" width=\"50%\"/>\n",
    "          <img src=\"10.png\" align=\"center\" width=\"50%\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. rating:\n",
    "\n",
    "* This column tells, if the user has clicked the offer or not (1:clicked, 0: not clicked, only viewed)\n",
    "* This is the target variable.\n",
    "* we see theat, there is **major class imbalance** in the data. Very few click-through (<5%), as compared to the large amount of non-click-through.\n",
    "* 95.5% of the cases, the click-through =0 (i.e. not clicked)\n",
    "* Only 4.5% of the cases, the click-through =1 (clicked)\n",
    "* Here we have shown the distribution of the rating for both clicked and not clicked.\n",
    "\n",
    "<img src=\"11.png\" align=\"left\" width=\"50%\"/> <img src=\"12.png\" align=\"right\" width=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Some additional features through feature engineering:\n",
    "\n",
    "* The three unique user IDs cannot be used as a feature in the ML models directly.\n",
    "* So, I am creating additional columns to gather the total click=1 information, corresponding each userID/offer ID/merchant ID.\n",
    "* So, for both train and test set, the columns we would consider for the Machine Learning are as follows:\n",
    "**'userid', 'offerid', 'category', 'merchant','dayofweek', 'date', 'hour', 'user_rating_sum', 'mer_rating_sum',\n",
    " 'off_rating_sum'**.\n",
    " \n",
    "* The target variable is **'rating'**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare the data for applying ML (Classification Algorithms):\n",
    "\n",
    "As we have discussed earlier, the dataset has huge class imbalance. So, there are different ways to tackle class imbalance: which are as follows:\n",
    "\n",
    "* **Resampling: oversample minority class:**  Good when you don't have a ton of data\n",
    "* **Resampling: undersample majority class:** Good when you have huge data\n",
    "* **Generate synthetic samples:** SMOTE (Synthetic minority oversampling technique)\n",
    "* **Class_weight:** This is one of the simple way to address the problem. The idea is to provide a weight for each class which places more emphasis on the minority classes such that the end result is a classifier which can learn equally from both the classes.\n",
    "\n",
    "Ref: https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling:\n",
    "\n",
    "As this is a classification problem. We will be applying i) **Random Forest** ii) **Gradient Boost** and iii) **Logistic Regression** and compare their performances.\n",
    "\n",
    "To trial with the models and find the best parameters, I will work with a random subset of the data. The original dataset(s) being too large, the computation is too time consuming, or actually not possible. (Given my computation power is very limited). \n",
    "\n",
    "The subset will be the represntative of the original dataset.\n",
    "\n",
    "\n",
    "### 6.1. Low volume dataset(s) : Random subsets:\n",
    "\n",
    "* Here we have taken the random subset (low volume) of the training and test datasets.\n",
    "* The low volume train data has 79224 rows and 12 columns.\n",
    "* The low volume test data has 9598 rows and 12 columns.\n",
    "* The new, low volume data is the representative of the main dataset.\n",
    "* I have performed the same EDA with the 'rating'feature again on this smaller dataset. Found the two plots (plotted with the big data and the small representative one) are exactly same.\n",
    "\n",
    "<img src=\"13.png\" align=\"left\" width=\"50%\"/> <img src=\"14.png\" align=\"right\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Applying ML algorithms and comparing their performances:\n",
    "\n",
    "This is a classification problem, in supervised learning. Here we have used the following classification models:\n",
    "\n",
    "* **Logistic Regression**\n",
    "* **Random Forest**\n",
    "* **Gradient Boost**\n",
    "\n",
    "Evaluating the performance of a model by training and testing on the same dataset can lead to the overfitting. Hence the model evaluation is based on splitting the dataset into train and validation set. However, the performance of the prediction result depends upon the random choice of the pair of (train, validation) set. To overcome, the Cross-Validation procedure is used where under the k-fold CV approach, the training set is split into k smaller sets, where a model is trained using k-1 of the folds as training data, and the model is validated on the remaining part.\n",
    "\n",
    "* **Classification/Confusion Matrix:** This matrix summarizes the correct and incorrect classifications that a classifier produced for a certain dataset. Rows and columns of the classification matrix correspond to the true and predicted classes respectively. The two diagonal cells (upper left, lower right) give the number of correct classifications, where the predicted class coincides with the actual class of the observation. The off diagonal cells gives the count of the misclassification. The classification matrix gives estimates of the true classification and misclassification rates.\n",
    "\n",
    "We applied different ML models above and evaluated their performances in terms of ROC-AUC score for both the training and test data. Here we have tabulated the scores and plotted them.\n",
    "\n",
    "<img src=\"score.png\" align=\"center\" width=\"50%\"/> \n",
    "\n",
    "<img src=\"17.png\" align=\"left\" width=\"50%\"/> <img src=\"18.png\" align=\"right\" width=\"50%\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Hyperparameter Tunning and the final two models:\n",
    "\n",
    "In Machine Learning, the hyperparameter tuning is choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is set before the learning process begins. This is significant as the performance of the entire model is based on the hyper parameter values specified. Some examples of hyperparameters include penalty in logistic regression and loss in stochastic gradient descent.\n",
    "\n",
    "different methods for optimizing hyperparameters: 1) Grid Search 2) Random Search\n",
    "\n",
    "**Grid search** is a traditional way to perform hyperparameter optimization. It works by searching exhaustively through a specified subset of hyperparameters. Using sklearn’s GridSearchCV, we first define our grid of parameters to search over and then run the grid search. Here I ran the GridSearchCV for both the **Random Forest** and **Gradient Boost**.\n",
    "\n",
    "* After fitting the models, with the tunned parameters, We improved our cross validation scores.\n",
    "\n",
    "* The ROC-AUC score for the final RF (Tunned) model is: 0.9008744808634841 and for GB (Tunned) model is: 0.9181525410415521\n",
    "\n",
    "* Performing a feature importance search reveals that, the engineered features are the most important ones.\n",
    "\n",
    "<img src=\"15.png\" align=\"left\" width=\"50%\"/> <img src=\"16.png\" align=\"right\" width=\"50%\"/> \n",
    "\n",
    "* Based on the low-volume training and test data, the final prediction table has been saved in the name **'final_result_rf.csv'** and **'final_result_GB.csv'**. This file has four columns. User-ID, Offer-ID, Merchant-ID, and the Rating.\n",
    "\n",
    "* \n",
    "\n",
    "* The ROC curves are as follows\n",
    "\n",
    "<img src=\"19.png\" align=\"left\" width=\"50%\"/> <img src=\"20.png\" align=\"right\" width=\"50%\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7. Conclusions:\n",
    "\n",
    "\n",
    "* The original dataset is enormous. (15844717 rows are there for the train set). Given my limited computational facility and time, it is almost impossible to deal with the Big-data. So, I have decided to take a random subset of it. This low-volume data would be easier to handle and representative of the population.\n",
    "\n",
    "* The 'country-code' feature has nothing to do with the analysis as the entire data belongs to only one country. So, I dropped the column.\n",
    "\n",
    "* The original, hence the low-volume data suffers from a significant class-imbalance problem. Though there are many ways to deal with it,  I have decided to use the 'class_weight' parameter in the sklearn for RF and the LR. The GB already takes care of the class imbalance problem.\n",
    "\n",
    "* To predict the ratings (probable clicks by the user), here I have considered a bunch of (7) features, either directly from the dataset or engineered/derived from the data. Interestingly, engineered features are the most important ones in terms of relative importance.\n",
    "\n",
    "* This is a **Classification** problem. Here we have used the following classification models:\n",
    "  * Logistic Regression\n",
    "  * Random Forest\n",
    "  * Gradient Boost\n",
    "\n",
    "* Evaluating the performance of a model by training and testing on the same dataset can lead to overfitting. Hence the model evaluation is based on splitting the dataset into train and validation set. But the performance of the prediction result depends upon the random choice of the pair of (train, validation) set. In order to overcome that, the **Cross-Validation** procedure is used where, under the k-fold CV approach, the training set is split into k smaller sets, where a model is trained using k-1 of the folds as training data, and the model is validated on the remaining part.\n",
    "\n",
    "* We have evaluated each model in terms of model accuracy score, precision, recall, f1, and the 'ROC-AUC' score for both the training and test data, and plotted them. The two best performing models are the Random forest and the Gradient boost. Both are the ensemble model based on decision trees.\n",
    "\n",
    "* Next, we have carried out the grid search CV for the hyperparameter tuning for both the models separately. This step was the most time consuming one in terms of computation. With the result of the optimized hyperparameters, we have again fitted the two models and got the predictions separately.\n",
    "\n",
    "* We have evaluated the ROC-AUC scores with the optimized hyperparameters. The model performance improved with the optimized parameters. The final ROC-AUC scores fro both RF and the GB are 0.901 and 0.918\n",
    "\n",
    "* The final prediction tables (Two columns: **User-ID**, **Offer-ID'**, **'Merchant-ID** and **Rating**) are saved as csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Future Direction:\n",
    "\n",
    "There is enough room to improve the model.\n",
    "\n",
    "* The first target would be to tackle the big-data. Given some resource (cloud computing platform, like AWS), the modeling needs to be done on the full train set.\n",
    "\n",
    "* To tackle the class-imbalance, other methods (including generating the synthetic set using SMOTE) needs to be tried and rested against each other.\n",
    "\n",
    "* Here we have used only the data of 14 days. The model can be improved if we can use more data.\n",
    "\n",
    "* Use ensembles of the machine learning models to average out bias and improve performance.\n",
    "\n",
    "* I wish there are more features in the data, as the age/sex of the users with different location information, along with some login information. These would have helped the modeling.\n",
    "\n",
    "* Try to fit and predict using the Extreme Gradient boost, classifier model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
